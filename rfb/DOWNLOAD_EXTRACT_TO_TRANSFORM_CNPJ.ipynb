{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libs\n",
    "import os\n",
    "import pandas as pd\n",
    "from download_transformacao_CNPJ import EXTRATOR_CNPJ\n",
    "from pyspark.sql.functions import concat_ws, lpad, coalesce, when, lit\n",
    "from time import localtime, strftime\n",
    "current_dir = os.getcwd()\n",
    "dir_dados = os.path.join(current_dir, r\"dados\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Começando a buscar os dados: 10/05/2023 09:01:28\n",
      "Termino da coleta dos ESTABELECIMENTOS: 10/05/2023 09:03:42\n",
      "Termino da coleta dos EMPRESAS: 10/05/2023 09:04:36\n",
      "Final da coleta dos dados: 10/05/2023 09:04:36\n"
     ]
    }
   ],
   "source": [
    "# Se passar baixar_e_extrair como false, precisa do nome do arquivo.\n",
    "print(f'Começando a buscar os dados: {strftime(\"%d/%m/%Y %H:%M:%S\", localtime())}')\n",
    "ESTABELECIMENTOS, spark = EXTRATOR_CNPJ(baixar_e_extrair=False, nome_arquivo=\"Estabelecimentos\").run()\n",
    "print(f'Termino da coleta dos ESTABELECIMENTOS: {strftime(\"%d/%m/%Y %H:%M:%S\", localtime())}')\n",
    "EMPRESAS, spark = EXTRATOR_CNPJ(baixar_e_extrair=False, nome_arquivo=\"Empresas\").run()\n",
    "print(f'Termino da coleta dos EMPRESAS: {strftime(\"%d/%m/%Y %H:%M:%S\", localtime())}')\n",
    "MUNICIPIOS, spark = EXTRATOR_CNPJ(baixar_e_extrair=False, nome_arquivo=\"Municipios\").run()\n",
    "print(f'Final da coleta dos dados: {strftime(\"%d/%m/%Y %H:%M:%S\", localtime())}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNAES = {\n",
    "        5612100:'Serviços ambulantes de alimentação',\n",
    "        5611201:'Restaurantes e similares',\n",
    "        5611203:'Lanchonetes casas de chá de sucos e similares',\n",
    "        5611204:'Bares e outros estabelecimentos especializados em servir bebidas sem entretenimento',\n",
    "        5611205:'Bares e outros estabelecimentos especializados em servir bebidas com entretenimento',\n",
    "        4721102: 'Padaria e confeitaria com predominância de revenda'\n",
    "        }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL QUERY SPARK\n",
    "``` PYTHON\n",
    "# Cria uma view com o mesmo nome do DataFrame\n",
    "ESTABELECIMENTOS.createOrReplaceTempView(\"ESTABELECIMENTOS\")\n",
    "# Contagem ativos, inativos e geral\n",
    "spark.sql(\"\"\"SELECT t1.CNAE_PRINCIPAL, t1.QUANTIDADE_FECHADA, t2.QUANTIDADE_GERAL, t3.QUANTIDADE_ABERTA\n",
    "            FROM (\n",
    "            SELECT CNAE_PRINCIPAL, COUNT(*) AS QUANTIDADE_FECHADA\n",
    "            FROM ESTABELECIMENTOS\n",
    "            WHERE CNAE_PRINCIPAL IN ('4721102','5612100', '5611201', '5611203', '5611204', '5611205') AND SITUACAO_CADASTRAL NOT IN (2, 3, 4)\n",
    "            GROUP BY CNAE_PRINCIPAL\n",
    "            ) AS t1\n",
    "            FULL OUTER JOIN (\n",
    "            SELECT CNAE_PRINCIPAL, COUNT(*) AS QUANTIDADE_GERAL\n",
    "            FROM ESTABELECIMENTOS \n",
    "            WHERE CNAE_PRINCIPAL IN ('4721102','5612100', '5611201', '5611203', '5611204', '5611205')\n",
    "            GROUP BY CNAE_PRINCIPAL \n",
    "            ) AS t2 \n",
    "            ON t1.CNAE_PRINCIPAL = t2.CNAE_PRINCIPAL\n",
    "            FULL OUTER JOIN (\n",
    "            SELECT CNAE_PRINCIPAL, COUNT(*) AS QUANTIDADE_ABERTA\n",
    "            FROM ESTABELECIMENTOS\n",
    "            WHERE CNAE_PRINCIPAL IN ('4721102','5612100', '5611201', '5611203', '5611204', '5611205') AND SITUACAO_CADASTRAL IN (2, 3, 4)\n",
    "            GROUP BY CNAE_PRINCIPAL\n",
    "            ) AS t3 \n",
    "            ON t1.CNAE_PRINCIPAL = t3.CNAE_PRINCIPAL\n",
    "            ORDER BY t1.CNAE_PRINCIPAL\n",
    "            LIMIT 100\n",
    "            \"\"\").show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtragem_cnae_sql(cod_cnae: int):\n",
    "    # Cria uma view com o mesmo nome do DataFrame\n",
    "    ESTABELECIMENTOS.createOrReplaceTempView(\"ESTABELECIMENTOS\")\n",
    "    EMPRESAS.createOrReplaceTempView(\"EMPRESAS\")\n",
    "    MUNICIPIOS.createOrReplaceTempView(\"MUNICIPIOS\")\n",
    "    # cria um dataframe com base na query\n",
    "    dataframe = spark.sql(\n",
    "        f\"\"\"\n",
    "        SELECT CONCAT(LPAD(EST.CNPJ_BASE, 8, '0'), LPAD(EST.CNPJ_ORDEM, 4, '0'), LPAD(EST.CNPJ_DV, 2, '0')) AS CNPJ,\n",
    "                EMP.RAZAO_SOCIAL,\n",
    "                EST.NOME_FANTASIA,\n",
    "                EST.SITUACAO_CADASTRAL,\n",
    "                EST.DATA_SITUACAO_CADASTRAL,\n",
    "                EST.DATA_INICIO_ATIVIDADE,\n",
    "                EST.CNAE_PRINCIPAL,\n",
    "                CONCAT(\n",
    "                    COALESCE(EST.TIPO_LOGRADOURO, ''),\n",
    "                    ' ',\n",
    "                    COALESCE(EST.LOGRADOURO, ''),\n",
    "                    ' ',\n",
    "                    COALESCE(EST.NUMERO, ''),\n",
    "                    ' ',\n",
    "                    COALESCE(EST.COMPLEMENTO, '')\n",
    "                ) AS ENDERECO\n",
    "                EST.BAIRRO,\n",
    "                MUN.NOME_MUNICIPIO AS CIDADE,\n",
    "                EST.UF,\n",
    "                EST.CEP,\n",
    "                CONCAT(\n",
    "                    COALESCE(EST.DDD_CONTATO, ''), \n",
    "                    ' ',\n",
    "                    COALESCE(EST.TELEFONE_CONTATO, '')\n",
    "                    ) AS TELEFONE,\n",
    "                EST.EMAIL\n",
    "        FROM ESTABELECIMENTOS AS EST, EMPRESAS AS EMP, MUNICIPIOS AS MUN\n",
    "        WHERE EST.CNPJ_BASE = EMP.CNPJ_BASE\n",
    "            AND EST.CODIGO_MUNICIPIO = MUN.CODIGO_MUNICIPIO\n",
    "            AND EST.CNAE_PRINCIPAL = {cod_cnae}\n",
    "            AND EST.SITUACAO_CADASTRAL IN (2, 3, 4)\n",
    "        \"\"\"\n",
    "    )\n",
    "    #display(dataframe.show(5))\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtragem_cnae_df(cod_cnae:int):\n",
    "    # cria um dataframe com base nos filtros aplicados\n",
    "    from pyspark.sql.functions import concat_ws, lpad, coalesce, when\n",
    "    dataframe = (\n",
    "        ESTABELECIMENTOS\n",
    "        .join(EMPRESAS, \"CNPJ_BASE\", \"right\")\n",
    "        .join(MUNICIPIOS, \"CODIGO_MUNICIPIO\", \"right\")\n",
    "        .where(\n",
    "            (ESTABELECIMENTOS[\"CNAE_PRINCIPAL\"] == f\"{cod_cnae}\") &\n",
    "            (ESTABELECIMENTOS[\"SITUACAO_CADASTRAL\"].isin([2, 3, 4]))\n",
    "        )\n",
    "        .select(\n",
    "            concat_ws(\"\", \n",
    "                lpad(ESTABELECIMENTOS[\"CNPJ_BASE\"].cast(\"bigint\"), 8, \"0\"), \n",
    "                lpad(ESTABELECIMENTOS[\"CNPJ_ORDEM\"], 4, \"0\"), \n",
    "                lpad(ESTABELECIMENTOS[\"CNPJ_DV\"], 2, \"0\")\n",
    "            ).alias(\"CNPJ\"),\n",
    "            when(EMPRESAS.RAZAO_SOCIAL.isNull(), None).otherwise(EMPRESAS.RAZAO_SOCIAL).alias(\"RAZAO_SOCIAL\"),\n",
    "            when(ESTABELECIMENTOS.NOME_FANTASIA.isNull(), None).otherwise(ESTABELECIMENTOS.NOME_FANTASIA).alias(\"NOME_FANTASIA\"),\n",
    "            ESTABELECIMENTOS.SITUACAO_CADASTRAL,\n",
    "            ESTABELECIMENTOS.DATA_SITUACAO_CADASTRAL,\n",
    "            ESTABELECIMENTOS.DATA_INICIO_ATIVIDADE,\n",
    "            ESTABELECIMENTOS.CNAE_PRINCIPAL,\n",
    "            concat_ws(\" \",\n",
    "                when(ESTABELECIMENTOS.TIPO_LOGRADOURO.isNull(), None).otherwise(ESTABELECIMENTOS.TIPO_LOGRADOURO),\n",
    "                when(ESTABELECIMENTOS.LOGRADOURO.isNull(), None).otherwise(ESTABELECIMENTOS.LOGRADOURO),\n",
    "                when(ESTABELECIMENTOS.NUMERO.isNull(), None).otherwise(ESTABELECIMENTOS.NUMERO),\n",
    "                when(ESTABELECIMENTOS.COMPLEMENTO.isNull(), None).otherwise(ESTABELECIMENTOS.COMPLEMENTO)\n",
    "            ).alias(\"ENDERECO\"),\n",
    "            ESTABELECIMENTOS.BAIRRO,\n",
    "            MUNICIPIOS.NOME_MUNICIPIO.alias(\"CIDADE\"),\n",
    "            ESTABELECIMENTOS.UF,\n",
    "            ESTABELECIMENTOS.CEP,\n",
    "            concat_ws(\"-\", \n",
    "                when(ESTABELECIMENTOS.DDD_CONTATO.isNull(), None).otherwise(ESTABELECIMENTOS.DDD_CONTATO),\n",
    "                when(ESTABELECIMENTOS.TELEFONE_CONTATO.isNull(), None).otherwise(ESTABELECIMENTOS.TELEFONE_CONTATO)\n",
    "            ).alias(\"TELEFONE\"),\n",
    "            ESTABELECIMENTOS.EMAIL\n",
    "        )\n",
    "    )\n",
    "\n",
    "    #display(dataframe.show(5))\n",
    "    return dataframe"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salva no disco e lê após isso\n",
    "``` PYTHON\n",
    "# Libs\n",
    "import requests # SE FOR USAR REQUESTS\n",
    "import urllib.request # SE FOR USAR URLLIB\n",
    "from pySmartDL import SmartDL # SE FOR USAR PYSMARTDL\n",
    "import zipfile\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Define ou busca uma sessão do Spark\n",
    "spark = SparkSession.builder.master(\"local[2]\") \\\n",
    "    .appName(\"OnlineReader\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define a url de download dos dados\n",
    "url = 'https://dadosabertos.rfb.gov.br/CNPJ/Simples.zip'\n",
    "# Pega o nome do arquivo pela url\n",
    "arquivo = url.split('/')[-1]\n",
    "# Define o caminho absoluto do diretório.\n",
    "salvar_onde = f\"{current_dir}/RAW/{arquivo.split('.')[0]}/\"\n",
    "\n",
    "# cria a pasta para armazenar o arquivo, se ela não existir\n",
    "if not os.path.exists(salvar_onde):\n",
    "    os.makedirs(salvar_onde)\n",
    "\n",
    "### Com requests\n",
    "\"\"\"\n",
    "    with requests.get(url, stream=True) as response:\n",
    "        with open(os.path.join(salvar_onde, arquivo), 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "\n",
    "\"\"\"\n",
    "### Com urllib\n",
    "\n",
    "# faz o download do arquivo e salva em salvar_onde/arquivo\n",
    "urllib.request.urlretrieve(url, os.path.join(salvar_onde, arquivo))\n",
    "\n",
    "### Com SmartDL\n",
    "\"\"\"\n",
    "    dest = os.path.join(salvar_onde, arquivo)\n",
    "    obj = SmartDL(url, dest, threads=4)\n",
    "    obj.start()\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "# Imprime o caminho do diretório de download\n",
    "print(salvar_onde)\n",
    "\"\"\"\n",
    "\n",
    "# Descompactação do arquivo\n",
    "with zipfile.ZipFile(os.path.join(salvar_onde, arquivo), 'r') as zip_ref:\n",
    "\n",
    "    # obtem o nome do primeiro arquivo dentro do zip\n",
    "    nome_original_arquivo_zip = zip_ref.namelist()[0]\n",
    "\n",
    "    # define um novo nome para o arquivo\n",
    "    novo_nome_arquivo = f\"CNPJ_{arquivo.split('.')[0]}.csv\"\n",
    "\n",
    "    # cria um dicionário com as informações de origem e destino\n",
    "    arquivos_para_extrair = {nome_original_arquivo_zip : novo_nome_arquivo}\n",
    "\n",
    "    # realiza a extração do arquivo zip\n",
    "    zip_ref.extractall(path = f\"{salvar_onde}/\", members=arquivos_para_extrair)\n",
    "    \n",
    "    # renomeia o arquivo extraído com o novo nome\n",
    "    os.rename(os.path.join(salvar_onde, nome_original_arquivo_zip), os.path.join(salvar_onde, novo_nome_arquivo))\n",
    "\n",
    "\"\"\"\n",
    "# imprime o nome do arquivo dentro do zip e o novo nome\n",
    "print(f\"Arquivo dentro do zip: {nome_original_arquivo_zip}\")\n",
    "print(f\"Novo nome do arquivo: {novo_nome_arquivo}\")\n",
    "\"\"\"\n",
    "\n",
    "# Define o caminho absoluto para o arquivo\n",
    "csv_file = os.path.join(salvar_onde, novo_nome_arquivo)\n",
    "\n",
    "# Lê o arquivo em um dataframe Spark\n",
    "dados = spark.read.options(delimiter=\";\", header=False, inferSchema=True).csv(csv_file)\n",
    "\n",
    "# Plota primeira linha do dataframe\n",
    "dados.show(1, vertical=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def salvar_df_cnae(CNAES:dict[int,str] = CNAES):\n",
    "    from backup_limpeza import backup_limpeza_simples\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        CNAES (dict[int,str], optional): informa um dicionário com os códigos e descrição cnae.\n",
    "    Return:\n",
    "        dados_pandas : salva o dataframe gerador pela função em um arquivo único csv e arquivos parquets\n",
    "    \"\"\"\n",
    "    arquivo_csv = os.path.join(dir_dados, r\"csv\\BASE_RFB.csv\")\n",
    "    if os.path.exists(arquivo_csv):\n",
    "        backup_limpeza_simples(pasta=arquivo_csv.replace(r\"BASE_RFB.csv\", \"\"), nome_zipado=f\"BASE_RFB_{strftime('%d-%m-%Y %H_%M_%S', localtime())}.zip\")\n",
    "    dados = None\n",
    "    for cod_cnae, descricao_cnae in CNAES.items():\n",
    "        print(cod_cnae)\n",
    "        dados = filtragem_cnae_df(cod_cnae)\n",
    "        dados = dados.withColumn(\n",
    "                        \"CNAE_DESCRICAO\", lit(descricao_cnae.upper())\n",
    "                    )\n",
    "        dados_pandas = dados.toPandas()\n",
    "        dados_pandas.to_csv(arquivo_csv, sep=\";\",mode=\"a\", encoding=\"utf-8\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5612100\n",
      "5611201\n",
      "5611203\n",
      "5611204\n",
      "5611205\n",
      "4721102\n"
     ]
    }
   ],
   "source": [
    "salvar_df_cnae(CNAES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "arquivo_csv = os.path.join(dir_dados, r\"csv\\BASE_RFB.csv\")\n",
    "print(os.path.exists(arquivo_csv))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define função que manipula a list ade arquivos\n",
    "```PYTHON\n",
    "def manipula_lista_arquivos(lista_de_arquivos:list):\n",
    "    # Lê cada arquivo parquet em um dataframe Spark \n",
    "    dados = None\n",
    "    for arquivo_no_diretorio in lista_de_arquivos:\n",
    "        if dados is None:\n",
    "            dados = (\n",
    "                spark.read.format(\"parquet\")\n",
    "                .option(\"inferSchema\", \"true\")\n",
    "                .load(arquivo_no_diretorio)\n",
    "            )\n",
    "        else:\n",
    "            # print(arquivo_no_diretorio)\n",
    "            dados_incrementados = (\n",
    "                spark.read.format(\"parquet\")\n",
    "                .option(\"inferSchema\", \"true\")\n",
    "                .load(arquivo_no_diretorio)\n",
    "            )\n",
    "            dados = dados.union(dados_incrementados)\n",
    "    \n",
    "    pd_dados = dados.toPandas()\n",
    "    #pd_dados = dados.toPandas()\n",
    "    return pd_dados```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` PYTHON\n",
    "dir_parquet = dir_dados + r\"\\parquet\"\n",
    "lista_arquivos_no_diretorio = [\n",
    "            os.path.join(dir_parquet, nome)\n",
    "            for nome in os.listdir(dir_parquet)\n",
    "            if nome.endswith(\".parquet\")\n",
    "        ]\n",
    "df_to_save = manipula_lista_arquivos(lista_arquivos_no_diretorio)```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```PYTHON\n",
    "arquivo_csv = os.path.join(dir_dados, r\"csv\\BASE_RFB.csv\")\n",
    "df_to_save.to_csv(arquivo_csv, sep=\";\", encoding=\"utf-8\", index=False)```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
